{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0235aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Music] hello everyone I am hu from Alibaba um today I'm glad to be here to talk about project lumx and show you why and how Dynamic scheduling should be employed in Lam influence serving lumni a joint effort by interns and colleagues from Alibaba let me begin with showing how ATMs are being served today from a cluster perspective LM services are typically deployed as multiple instances of the model running inside a certain INF engine with a request dispatcher top ring the incoming requests to the instances um for the INF engine ler we have seen a lot of systems tored for LMS including those you've just heard of in the previous talks and these systems show Superior performance inside a single instance for the dispatcher people are still using um policies or scheduling systems inherited from the era of traditional DNS but not designed for LMS but the problem is um LMS are different and require new design Philosophy for the scheding layer the first characteristic of llm is that workloads are heterogeneous um LMS are un Universal models in a sense that the same model can work for different tasks with um context specific inputs provided so that means nlms can have diverse applications and therefore heterogeneous requests first um request will have different input and output LS um consider a simple intuitive example that you want to use an llm to summarize an article or to um write an article from a simple prompt or to an article you've just written so these requests will have drastically different ranges of input and output lens another aspect is latency slos for example um interactive applications like chatbots or personal assistant we would expect lower latencies than offline analysis tasks like summarize an article another real product example is that open AI introduced a subscription Plan called chat GPT plus which promises faster responses and namely lower latencies the heterogenity of requests becomes more challenging um when combined with another characteristic um which I believe you have been familiar with after the previous talks um namely the unpredictable autor regressive execution during the auto regressive execution the final output lengths are not known a prior which also come with the the GPU memory demands for KV caches that's that dynamically grow with sequences so state-of-the-art systems use page detention for uh dynamic memory allocation for KV caches and combines it with preemptive scheduling uh cuz you know if you cannot know the final K catch sizes in the beginning then you might have to preempt certain requests when you run off the memory these characteristics introduce a series of scheduling challeng es that affect the the performance and quality of line services the first is performance isolation so preemptions are inevitable when you use dynamic memory allocation and they can cause poor tail latencies and SLO violations because the preempted requests will have to go through an another round of queuing even without preemptions the requests in the batch also have performance interference to each other so obviously we need load balancing to um load balancing across in instances to mitigate such issues but the point is load balancing via the one shot dispatching could be suboptimal because you cannot know the final output lengths in the beginning so U that means we require continuous load balancing even after the requests are dispatched is that enough no load balancing also means memory fragmentation AC Crossing instances because we tend to spread the cluster's free memory space across multiple instances um actually this is a classic spreading versus packing trade-off in scheduling problems fragmentation could lead to worse curing delays especially for the requests with long inputs because a large space on one instance is needed for accommodating the Cave cach of the whole prompt to visualize this we did an experiment with four instances using a spreading policy for request dispatching we plot the memory demand of the head of line curing request on each instance against the total free memory space in a cluster for most of the time span the total memory space can satisfy um the queueing request on at least three instances but they're still queuing because the free space is fragmented this shows that Beyond load balancing we also need the ability of fragmentation a third challenge is to satisfy different latency slos of requests existing systems treat all requests equally where urgent requests could easily be interfered by normal ones for example they may experience excessive cin delays or performance interference when collocated with normal requests so we also need request priorities to systematically differentiate differentiate their SL okay in a nutshell the major takeaway here is that LMS are inherently multi-tenant and dynamic serving heterogeneous and unpredictable workloads on multiple instances this behavior is actually quite different from traditional DNN where the requests are mostly homogeneous and the execution is deterministic and stateless but luckily this behavior is not new in modern operating systems or distributed systems and in these systems we have um Contex switching we have process migration and many other similar approaches to deal with such multi-tenancy and dynamism related problems so how about LMS our answer to this question is lumx a system that features continuous rescheduling of llm requests across multiple model instances it is bined with the initial dispatching of requests and instance Autos scaling we show that rescheduling is powerful weapon in various scheduling scenarios the first is rescheduling for load balancing will continuous load balancing um to reduce preemptions and interference um which is complimentary to the dispatching time load balancing to tame the trade-off between load balancing and and fragmentation lumx also reschedules requests away from an instance to create free space for defragmentation to reduce queueing delays the third scenario is rescheduling requests away from high priority requests to further reduce performance interference and accelerate the execution of high priority requests and finally um lumx also reschedules quests to saturate or drain out instances more quickly during Autos scaling so you might have heard of res scheduling and or migration of LM requests as ya just introduced in server as LM and I believe that this also shows the value and potential of res scheding well our aim is instead to realize fully Dynamic scheduling and make rescheduling the norm not exception in llm serving to this end we need to accomplish a series of design goals we need to maximize the efficiency of the rescheduling so that we could reschedule much more aggressively and more frequently we also need a high scalability of a cluster level scheduler and a scheduling policy that maximizes the scheduling benefits of rescheduling in various scheduling goals our first introduce the live migration mechanism so the major concern in um with rescheduling is the KV cach States you could either recompute or copy the KV cach to new instance but this approach will introduce down times to the rescheduled requests um for the recomputing or memory copying and also performance overhead due to the re redundant computation of um recomputing you know this recomputing will need another prefi computation round on the destination instance which will block the original batch on the destination instance so I guess this is the key difference between lumx and server well more importantly these downtimes and overheads will increase with the sequence lens but by contrast um lum's live migration mechanism has near zero downtime and overhead by Design the inspiration is virtual machine live migration when you want to migrate VM you keep the source VM online during which you iteratively copy the 30 pages namely the pages got changed to the destination VM when all the 30 pages get synced then the source VM stops the migration is committed and the new VM becomes online to apply this technique to LM serving we can liken the VMS to requests and the memory pages to KV blocks now the question is what are the dirty Pages the key characteristic of anlm inference here is that KV caches are aend only that means there are no dirty Pages or dirty blocks and there are only incremental blocks that are appended to the cache as the as the decoding precedes the live migration in lumx Works in multiple stages where we iteratively copy the incremental blocks um to the generated in the last stage um for the first stage they are blocks generated before the migration and two we reach a stage that only produces the minimum number of new blocks and so we um uh namely the stage end in graph so we will re we will suspend the request copy the last box and resume the request on the destination instance so this way the migration downtime is that only that of the final copying which is near zero and constant to the sequence L with live migration being the mechan mechanism Foundation I'll next show you how lumx improves the scalability and scheding benefits of migration continuous rescheduling cross instances also implies higher scheduling pressure than traditional schedules which could be a bottleneck if implemented in a centralized manner LX uses uses a distributed scheduling architecture that employs a global scheduler for cross instance scheduling decisions while using a set of distributed schedulers named LLS collocated with the model instances to handle intra instance scheduling in this architecture the global scheduler makes all the scheduling decisions only based on the memory loads of the instances reported by the Lums it does not need to watch the status of every single running request which further reduces its pressure the key enabler of our Global schedule not caring about the specific request is our scheduling policy which translates the various scheduling goals into a simple instance load metric we achieve this with an abstraction called the virtual usage the int the intuition is that the various goals are all controlling the loads of the instances you either balance them or you create more free space on certain instances they can actually be unified into load balancing because for creating free space um we can we can simply assume a virtual load on that instance to make it virtually overloaded then the load balancing policy will be triggered to migrate some running requests to other instances so lamex uses a load balancing policy based on Virtual usages and Define the rules for setting virtual usages in different scenarios for example when we have a queueing request lumni assumes the positive virtual usage of it although the physical usage is zero so that we can migrate other Quest away which in fact is defragmentation and this approach also works for other scenarios and you can refer to our paper for details lumx is implemented as a scheding layer on top of multiple instances of VM uh stateof the-art inference engine we evaluate lumx on the cluster with 16 A10 gpus using the popular llama models and we use two real data sets of chat gb4 conversations for the sequence lengths and also generated power law distributions to represent more diverse applications we compare the serving performance of lumni with infas a system that implements explicit load balancing for dispatching on the share GPT and burst GPT data sets lomnick achieves up to 2.2 times and 5. five times gains for mean and P99 first token latencies via defragmentation to reduce cooling days lumx also shows up to 1.3 times shorter P99 per token Generation latencies by reducing preemptions with migration on average lumx reduces latency penalties caused by preemptions by 73% across this instance um across these experiments um due to the time limit I will refer you to our paper for other results like a benchmark for the migration efficiency and other chases priorities and Autos scaling okay to conclude we believe that Dynamic workloads need Dynamic scheduling and ATMs are no exception we built lumni exactly following this principle the design of lumni draws lessons from conventional systems wisdom including definitions of classic scheduling goals in the new context of llm serving implementation of the rescheduling with the live migration mechanism and fully continuous Dynamic rescheduling exploiting the migration combined this techniques deliver better latencies and efficiency of LM serving lumni is open source and welcome to take a try okay thank you and I'm happy to take any questions [Applause] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 161\n",
    "infile_path = str(num) + \".txt\"\n",
    "outfile_path = str(num) + \"out.txt\"\n",
    "outstr = \"\"\n",
    "with open(infile_path, 'r', encoding=\"utf-8\") as file:\n",
    "    i = -1\n",
    "    for line in file:\n",
    "        i += 1\n",
    "        if i % 2 == 1:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            outstr += \"\\n\"\n",
    "            # print()\n",
    "        else:\n",
    "            outstr += line\n",
    "            outstr += ' '\n",
    "            # print(line, end=\"\")\n",
    "print(outstr)\n",
    "with open(outfile_path, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(outstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
